# Training Data Generation Examples
# This file demonstrates how to generate and manage training data in Runa

import runa.ai.training_data
import runa.ai.data_processing

# Example 1: Basic training data generation from existing code
Process called "generate_basic_training_data":
    # Initialize the generator
    Let generator = CodeCorpusGenerator.create({
        "output_directory": "./training_data/",
        "file_format": "jsonl"
    })
    
    # Generate corpus from existing codebase
    Print("Generating corpus from codebase...")
    Let corpus = generator.from_codebase("./src/", {
        "file_extensions": [".runa"],
        "exclude_patterns": ["**/test/**"],
        "sample_count": 1000
    })
    
    # Export the corpus
    corpus.export("./training_data/runa_code_corpus.jsonl")
    
    Print("Generated " + corpus.count + " examples")
    Return corpus

# Example 2: Generating synthetic examples
Process called "generate_synthetic_examples":
    # Initialize the generator
    Let generator = CodeCorpusGenerator.create()
    
    # Define code patterns to generate
    Let patterns = [
        {
            "type": "function_definition",
            "complexity": "medium",
            "parameters_count": [1, 5],
            "includes_return": true
        },
        {
            "type": "conditional_block",
            "nested_levels": [1, 3],
            "includes_else": true
        },
        {
            "type": "loop_construct",
            "variants": ["for", "while"],
            "complexity": "high"
        }
    ]
    
    # Generate synthetic examples
    Print("Generating synthetic examples...")
    Let synthetic_corpus = generator.generate_synthetic({
        "patterns": patterns,
        "count": 500,
        "diversity_level": 0.8
    })
    
    # Export the synthetic corpus
    synthetic_corpus.export("./training_data/synthetic_examples.jsonl")
    
    Print("Generated " + synthetic_corpus.count + " synthetic examples")
    Return synthetic_corpus

# Example 3: Data augmentation for training data
Process called "augment_training_data" that takes corpus:
    # Initialize the augmenter
    Let augmenter = DataAugmenter.create()
    
    # Define augmentation techniques
    Let techniques = [
        {
            "type": "variable_renaming",
            "probability": 0.7,
            "preserve_semantics": true
        },
        {
            "type": "comment_variation",
            "probability": 0.5,
            "preserve_intent": true
        },
        {
            "type": "code_restructuring",
            "probability": 0.3,
            "transformations": ["extract_function", "inline_function", "reorder_statements"]
        },
        {
            "type": "style_variation",
            "probability": 0.4,
            "styles": ["concise", "verbose", "functional"]
        }
    ]
    
    # Apply augmentation
    Print("Augmenting training data...")
    Let augmented_corpus = augmenter.transform(corpus, techniques)
    
    # Export the augmented corpus
    augmented_corpus.export("./training_data/augmented_corpus.jsonl")
    
    Print("Generated " + augmented_corpus.count + " augmented examples")
    Return augmented_corpus

# Example 4: Filtering and quality assessment
Process called "assess_and_filter_data" that takes corpus:
    # Initialize the quality assessor
    Let assessor = DataQualityAssessor.create()
    
    # Define quality criteria
    Let quality_criteria = {
        "metrics": ["diversity", "correctness", "completeness", "uniqueness"],
        "min_scores": {
            "diversity": 0.6,
            "correctness": 0.8,
            "completeness": 0.7,
            "uniqueness": 0.9
        },
        "syntax_check": true,
        "lint_level": "strict"
    }
    
    # Assess quality
    Print("Assessing data quality...")
    Let quality_report = assessor.evaluate(corpus, quality_criteria)
    
    # Print quality report summary
    Print("Quality Report:")
    Print("- Overall Score: " + quality_report.overall_score)
    For metric in quality_report.metrics:
        Print("- " + metric.name + ": " + metric.score)
    
    # Filter based on quality
    Print("Filtering low-quality examples...")
    Let filtered_corpus = assessor.filter(corpus, {
        "min_quality_score": 0.7,
        "exclude_duplicates": true,
        "exclude_syntax_errors": true
    })
    
    Print("Retained " + filtered_corpus.count + " high-quality examples out of " + corpus.count)
    Return filtered_corpus

# Example 5: Creating instruction fine-tuning datasets
Process called "create_instruction_dataset" that takes corpus:
    # Initialize the fine-tuning dataset generator
    Let ft_generator = FineTuningDataGenerator.create()
    
    # Define instruction templates
    Let instruction_templates = [
        {
            "pattern": "Write a function that {{task}}",
            "task_examples": [
                "calculates the factorial of a number",
                "sorts a list using quicksort algorithm",
                "finds prime numbers in a given range",
                "validates user input for a form"
            ]
        },
        {
            "pattern": "How would you implement {{feature}} in Runa?",
            "feature_examples": [
                "a cache system",
                "error handling for network requests",
                "a simple web server",
                "multithreading for parallel processing"
            ]
        },
        {
            "pattern": "Fix the bug in this code: {{buggy_code}}",
            "extract_buggy_samples": true
        }
    ]
    
    # Generate instruction dataset
    Print("Creating instruction dataset...")
    Let instruct_dataset = ft_generator.create_instruction_dataset({
        "code_corpus": corpus,
        "instruction_templates": instruction_templates,
        "count": 500,
        "include_metadata": true
    })
    
    # Export the instruction dataset
    instruct_dataset.export("./training_data/instruction_dataset.jsonl")
    
    Print("Generated " + instruct_dataset.count + " instruction examples")
    Return instruct_dataset

# Example 6: Dataset splitting
Process called "split_dataset" that takes corpus:
    # Initialize the dataset splitter
    Let splitter = DatasetSplitter.create()
    
    # Split dataset for model training
    Print("Splitting dataset...")
    Let splits = splitter.split(corpus, {
        "train": 0.8,
        "validation": 0.1,
        "test": 0.1,
        "stratify_by": "complexity"
    })
    
    # Validate the distribution across splits
    Let distribution_report = splitter.validate_distribution(splits, [
        "pattern_types", "complexity", "token_count"
    ])
    
    # Print distribution report
    Print("Distribution Report:")
    For feature in distribution_report.features:
        Print("- " + feature.name + " distribution p-value: " + feature.p_value)
    
    # Export splits
    Print("Exporting splits...")
    For split_name in ["train", "validation", "test"]:
        Let output_file = "./dataset_splits/" + split_name + ".jsonl"
        splits[split_name].export(output_file)
        Print("- " + split_name + ": " + splits[split_name].count + " examples")
    
    Return splits

# Example 7: Full training data pipeline
Process called "complete_training_data_pipeline":
    # Initialize all components
    Let generator = CodeCorpusGenerator.create()
    Let augmenter = DataAugmenter.create()
    Let assessor = DataQualityAssessor.create()
    Let ft_generator = FineTuningDataGenerator.create()
    Let splitter = DatasetSplitter.create()
    Let versioner = DataVersioner.create("./data_repository/")
    
    # Step 1: Generate from codebase
    Print("Step 1: Generating base corpus...")
    Let base_corpus = generator.from_codebase("./src/", {
        "file_extensions": [".runa"],
        "exclude_patterns": ["**/test/**"]
    })
    
    # Step 2: Generate synthetic examples
    Print("Step 2: Generating synthetic examples...")
    Let synthetic_corpus = generator.generate_synthetic({
        "patterns": [
            {"type": "function_definition", "complexity": "medium"},
            {"type": "conditional_block", "nested_levels": [1, 3]},
            {"type": "loop_construct", "variants": ["for", "while"]}
        ],
        "count": 1000
    })
    
    # Step 3: Combine corpora
    Print("Step 3: Combining corpora...")
    Let combined_corpus = generator.combine([base_corpus, synthetic_corpus])
    
    # Step 4: Augment data
    Print("Step 4: Augmenting data...")
    Let augmented_corpus = augmenter.transform(combined_corpus, [
        {"type": "variable_renaming", "probability": 0.5},
        {"type": "comment_variation", "probability": 0.3},
        {"type": "code_restructuring", "probability": 0.2}
    ])
    
    # Step 5: Quality assessment
    Print("Step 5: Assessing quality...")
    Let quality_report = assessor.evaluate(augmented_corpus)
    
    # Step 6: Filter data
    Print("Step 6: Filtering data...")
    Let filtered_corpus = assessor.filter(augmented_corpus, {
        "min_quality_score": 0.7,
        "exclude_duplicates": true
    })
    
    # Step 7: Create instruction dataset
    Print("Step 7: Creating instruction dataset...")
    Let instruct_dataset = ft_generator.create_instruction_dataset({
        "code_corpus": filtered_corpus,
        "count": 1000
    })
    
    # Step 8: Create completion dataset
    Print("Step 8: Creating completion dataset...")
    Let completion_dataset = ft_generator.create_completion_dataset({
        "code_corpus": filtered_corpus,
        "context_length": 200,
        "completion_length": 50,
        "count": 1000
    })
    
    # Step 9: Combine datasets
    Print("Step 9: Combining fine-tuning datasets...")
    Let ft_dataset = ft_generator.combine([instruct_dataset, completion_dataset])
    
    # Step 10: Split dataset
    Print("Step 10: Splitting dataset...")
    Let splits = splitter.split(ft_dataset, {
        "train": 0.8,
        "validation": 0.1,
        "test": 0.1
    })
    
    # Step 11: Version dataset
    Print("Step 11: Versioning dataset...")
    Let version_id = versioner.commit(splits, {
        "version_name": "runa_finetune_v1",
        "description": "Runa code corpus for fine-tuning"
    })
    
    # Step 12: Export final datasets
    Print("Step 12: Exporting final datasets...")
    For split_name in ["train", "validation", "test"]:
        Let output_file = "./final_dataset/" + split_name + ".jsonl"
        splits[split_name].export(output_file)
    
    # Return pipeline results
    Return {
        "version_id": version_id,
        "stats": {
            "base_examples": base_corpus.count,
            "synthetic_examples": synthetic_corpus.count,
            "augmented_examples": augmented_corpus.count,
            "filtered_examples": filtered_corpus.count,
            "final_examples": ft_dataset.count,
            "quality_score": quality_report.overall_score
        }
    }

# Example 8: Domain-specific data generation
Process called "generate_domain_data" that takes domain:String:
    # Initialize domain-specific generator
    Let domain_generator = DomainDataGenerator.create(domain)
    
    Print("Generating " + domain + " domain examples...")
    
    # Load domain templates
    domain_generator.load_templates("./domain_templates/" + domain + "/")
    
    # Generate domain-specific examples
    Let domain_examples = domain_generator.generate({
        "count": 500,
        "include_domain_entities": true,
        "complexity_range": [1, 5]
    })
    
    # Export domain examples
    domain_examples.export("./training_data/" + domain + "_examples.jsonl")
    
    Print("Generated " + domain_examples.count + " " + domain + " examples")
    Return domain_examples 