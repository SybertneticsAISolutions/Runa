# Model Fine-tuning Examples
# This file demonstrates how to use Runa's model fine-tuning capabilities

import runa.ai.model_tuning
import runa.ai.datasets
import runa.ai.evaluation

# Example 1: Basic supervised fine-tuning
Process called "basic_supervised_fine_tuning":
    # Step 1: Load a pre-trained model
    Print("Loading pre-trained model...")
    Let model_loader = ModelLoader.create({
        "model_type": "llm",
        "model_id": "runa-base-7b",
        "device": "auto"
    })
    
    Let base_model = model_loader.load_for_fine_tuning({
        "adapter_type": "lora",
        "trainable_components": ["attention"],
        "freeze_embeddings": true,
        "rank": 8
    })
    
    # Step 2: Prepare training data
    Print("Preparing training data...")
    Let dataset_config = DatasetConfigurator.create({
        "format": "instruction",
        "validation_split": 0.1
    })
    
    # Add code examples as data source
    dataset_config.add_data_source("./examples/code_samples/", {
        "format": "code",
        "recursive": true,
        "file_extensions": [".runa"]
    })
    
    # Add instruction data
    dataset_config.add_data_source("./examples/instructions.jsonl", {
        "format": "jsonl",
        "keys": {
            "input": "instruction",
            "output": "response"
        }
    })
    
    # Create the final datasets
    Let datasets = dataset_config.prepare_datasets()
    Print("Training examples: " + datasets.train.count)
    Print("Validation examples: " + datasets.validation.count)
    
    # Step 3: Configure training
    Print("Configuring training...")
    Let training_config = TrainingConfigurator.create({
        "training_objective": "supervised",
        "optimization": {
            "optimizer": "adamw",
            "learning_rate": 2e-5,
            "weight_decay": 0.01,
            "lr_scheduler": "cosine",
            "warmup_steps": 50
        },
        "training_parameters": {
            "batch_size": 4,
            "gradient_accumulation_steps": 4,
            "epochs": 2
        }
    })
    
    # Set evaluation metrics
    training_config.add_evaluation_metric("loss")
    training_config.add_evaluation_metric("perplexity")
    
    # Step 4: Run fine-tuning
    Print("Starting fine-tuning...")
    Let fine_tuner = FineTuner.create({
        "model": base_model,
        "datasets": datasets,
        "training_config": training_config,
        "output_dir": "./models/basic_fine_tuned/"
    })
    
    Let training_result = fine_tuner.train()
    
    # Step 5: Evaluate the model
    Print("Evaluating fine-tuned model...")
    Let evaluator = ModelEvaluator.create({
        "model": training_result.model,
        "datasets": {"validation": datasets.validation}
    })
    
    Let eval_results = evaluator.evaluate()
    
    # Print results
    Print("Fine-tuning completed!")
    Print("Training loss: " + training_result.metrics.final_training_loss)
    Print("Validation loss: " + eval_results.validation.loss)
    Print("Validation perplexity: " + eval_results.validation.perplexity)
    
    Return {
        "model": training_result.model,
        "metrics": training_result.metrics,
        "evaluation": eval_results
    }

# Example 2: Advanced fine-tuning with domain adaptation
Process called "domain_adaptation_fine_tuning":
    # Step 1: Load base model
    Print("Loading base model...")
    Let model_loader = ModelLoader.create({
        "model_type": "llm",
        "model_id": "runa-base-7b"
    })
    
    Let base_model = model_loader.load_for_fine_tuning({
        "adapter_type": "lora",
        "rank": 16,
        "alpha": 32
    })
    
    # Step 2: Configure domain adaptation
    Print("Configuring domain adaptation...")
    Let domain_config = DomainAdaptationConfigurator.create({
        "base_model": base_model,
        "domain": "financial_programming",
        "adaptation_method": "continued_pretraining"
    })
    
    # Add domain-specific data
    domain_config.add_domain_data("./domain_data/financial_code/", {
        "recursive": true,
        "file_extensions": [".runa", ".py", ".js"]
    })
    
    domain_config.add_domain_data("./domain_data/financial_docs.jsonl", {
        "format": "jsonl",
        "weight": 0.3
    })
    
    # Step 3: Run domain adaptation
    Print("Running domain adaptation...")
    Let domain_adapter = DomainAdapter.create({
        "config": domain_config,
        "output_dir": "./models/domain_adapted/"
    })
    
    Let domain_result = domain_adapter.adapt()
    
    # Step 4: Task-specific fine-tuning on the domain-adapted model
    Print("Preparing task-specific datasets...")
    Let task_dataset_config = DatasetConfigurator.create({
        "format": "instruction",
        "validation_split": 0.1
    })
    
    task_dataset_config.add_data_source("./task_data/financial_instructions.jsonl", {
        "format": "jsonl",
        "keys": {
            "input": "instruction",
            "output": "response"
        }
    })
    
    Let task_datasets = task_dataset_config.prepare_datasets()
    
    # Configure task fine-tuning
    Print("Configuring task-specific fine-tuning...")
    Let task_training_config = TrainingConfigurator.create({
        "training_objective": "supervised",
        "optimization": {
            "optimizer": "adamw",
            "learning_rate": 1e-5,  # Lower learning rate for secondary fine-tuning
            "weight_decay": 0.01
        },
        "training_parameters": {
            "batch_size": 4,
            "epochs": 1
        }
    })
    
    # Run task fine-tuning
    Print("Running task-specific fine-tuning...")
    Let task_fine_tuner = FineTuner.create({
        "model": domain_result.model,
        "datasets": task_datasets,
        "training_config": task_training_config,
        "output_dir": "./models/task_fine_tuned/"
    })
    
    Let task_result = task_fine_tuner.train()
    
    # Step 5: Evaluate on domain-specific test data
    Print("Evaluating domain-adapted and task-tuned model...")
    Let domain_test_data = "./evaluation_data/financial_test_cases.jsonl"
    
    Let domain_evaluator = ModelEvaluator.create({
        "models": {
            "base": base_model,
            "domain_adapted": domain_result.model,
            "task_tuned": task_result.model
        },
        "datasets": {"domain_test": domain_test_data}
    })
    
    Let comparative_results = domain_evaluator.evaluate_all()
    
    # Print comparison
    Print("Comparative evaluation results:")
    For model_name in comparative_results.keys():
        Print("Model: " + model_name)
        Let model_results = comparative_results[model_name]
        Print("- Loss: " + model_results.domain_test.loss)
        Print("- Perplexity: " + model_results.domain_test.perplexity)
    
    Return {
        "domain_adapted_model": domain_result.model,
        "task_tuned_model": task_result.model,
        "evaluation": comparative_results
    }

# Example 3: RLHF (Reinforcement Learning from Human Feedback)
Process called "rlhf_fine_tuning":
    # Step 1: Start with a supervised fine-tuned model
    Print("Loading supervised fine-tuned model...")
    Let model_loader = ModelLoader.create()
    Let supervised_model = model_loader.load("./models/supervised_fine_tuned/")
    
    # Step 2: Configure RLHF
    Print("Configuring RLHF...")
    Let rlhf_config = RLHFConfigurator.create({
        "supervised_model": supervised_model,
        "reward_model_type": "auto_create",
        "rl_algorithm": "ppo",
        "training_parameters": {
            "kl_penalty": 0.1,
            "reward_scale": 0.1,
            "ppo_epochs": 4,
            "max_steps": 1000
        }
    })
    
    # Add comparison data for training the reward model
    rlhf_config.add_comparison_data("./feedback_data/comparisons.jsonl", {
        "format": "jsonl",
        "keys": {
            "prompt": "instruction",
            "better": "preferred_response",
            "worse": "rejected_response"
        }
    })
    
    # Step 3: Run RLHF training
    Print("Running RLHF training...")
    Let rlhf_trainer = RLHFTrainer.create({
        "config": rlhf_config,
        "output_dir": "./models/rlhf_tuned/"
    })
    
    Let rlhf_result = rlhf_trainer.train()
    
    # Step 4: Evaluate RLHF model against supervised model
    Print("Evaluating RLHF-tuned model...")
    Let rlhf_evaluator = ModelEvaluator.create({
        "models": {
            "supervised": supervised_model,
            "rlhf": rlhf_result.model
        },
        "datasets": {"test": "./evaluation_data/rlhf_test_cases.jsonl"},
        "metrics": ["win_rate", "human_eval"]
    })
    
    Let rlhf_eval_results = rlhf_evaluator.evaluate_all()
    
    # Print results
    Print("RLHF evaluation results:")
    Print("Win rate of RLHF vs Supervised: " + 
          rlhf_eval_results.rlhf.test.win_rate_vs_supervised)
    Print("Human eval score (RLHF): " + rlhf_eval_results.rlhf.test.human_eval)
    Print("Human eval score (Supervised): " + rlhf_eval_results.supervised.test.human_eval)
    
    Return {
        "rlhf_model": rlhf_result.model,
        "evaluation": rlhf_eval_results
    }

# Example 4: Model merging and quantization
Process called "model_merging_and_quantization":
    # Step 1: Load multiple specialized models
    Print("Loading specialized models...")
    Let model_loader = ModelLoader.create()
    
    Let models = {
        "code_generation": model_loader.load("./models/code_gen_model/"),
        "documentation": model_loader.load("./models/documentation_model/"),
        "debugging": model_loader.load("./models/debugging_model/")
    }
    
    # Step 2: Configure model merging
    Print("Configuring model merging...")
    Let merger = ModelMerger.create({
        "base_model": "runa-base-7b",
        "merge_method": "slerp",
        "merge_parameters": {
            "interpolation_weights": {
                "code_generation": 0.4,
                "documentation": 0.3,
                "debugging": 0.3
            },
            "merge_strategy": "layer_wise"
        }
    })
    
    # Add models to merge
    For model_name in models.keys():
        merger.add_model(model_name, models[model_name])
    
    # Step 3: Execute the merge
    Print("Merging models...")
    Let merged_model = merger.merge()
    
    # Step 4: Quantize the merged model
    Print("Quantizing the merged model...")
    Let optimizer = ModelOptimizer.create({
        "model": merged_model,
        "optimization_targets": ["size", "latency"],
        "target_hardware": "cpu"
    })
    
    # Analyze and apply quantization
    Let analysis = optimizer.analyze_optimization_potential()
    Print("Recommended quantization: " + analysis.recommended_quantization)
    
    Let quantized_model = optimizer.quantize({
        "method": analysis.recommended_quantization
    })
    
    # Step 5: Evaluate original vs quantized model
    Print("Evaluating original and quantized models...")
    Let test_data = "./evaluation_data/mixed_test_cases.jsonl"
    
    Let quant_evaluator = ModelEvaluator.create({
        "models": {
            "merged": merged_model,
            "quantized": quantized_model
        },
        "datasets": {"test": test_data}
    })
    
    Let quant_results = quant_evaluator.evaluate_all()
    
    # Step 6: Benchmark performance
    Print("Benchmarking performance...")
    Let benchmark = optimizer.benchmark(quantized_model, {
        "test_inputs": "./evaluation_data/benchmark_inputs.jsonl",
        "iterations": 50,
        "warmup_iterations": 5
    })
    
    # Print results
    Print("Model size comparison:")
    Print("Original merged model: " + benchmark.original_size + " MB")
    Print("Quantized model: " + benchmark.optimized_size + " MB")
    Print("Size reduction: " + benchmark.size_reduction + "%")
    
    Print("Performance comparison:")
    Print("Average latency improvement: " + benchmark.latency_improvement + "%")
    Print("Accuracy difference: " + 
          (quant_results.merged.test.exact_match - quant_results.quantized.test.exact_match))
    
    Return {
        "merged_model": merged_model,
        "quantized_model": quantized_model,
        "benchmark": benchmark,
        "evaluation": quant_results
    }

# Example 5: Creating a custom dataset from code repositories
Process called "create_custom_training_dataset":
    # Step 1: Configure data source manager
    Print("Configuring data sources...")
    Let data_sources = DataSourceManager.create()
    
    # Add a code repository
    data_sources.add_code_repository("./src/", {
        "file_extensions": [".runa"],
        "exclude_patterns": ["**/test/**", "**/examples/**"],
        "code_extraction": "function",
        "include_comments": true,
        "min_tokens": 20,
        "max_tokens": 1024
    })
    
    # Add documentation
    data_sources.add_documentation("./docs/", {
        "file_extensions": [".md"],
        "chunk_size": "section"
    })
    
    # Add manually created examples
    data_sources.add_instruction_data("./training_data/manual_examples.jsonl")
    
    # Step 2: Process and prepare the data
    Print("Processing data sources...")
    Let data_processor = TrainingDataProcessor.create({
        "data_sources": data_sources,
        "output_format": "instruction",
        "output_directory": "./processed_data/",
        "deduplicate": true,
        "validation_split": 0.1,
        "test_split": 0.1
    })
    
    Let processed_data = data_processor.process()
    
    # Step 3: Generate synthetic examples
    Print("Generating synthetic examples...")
    Let synthetic_generator = SyntheticDataGenerator.create({
        "seed_examples": processed_data.train.sample(100),
        "variation_strategies": ["paraphrase", "complexity_variation", "edge_case_generation"],
        "count": 500
    })
    
    Let synthetic_examples = synthetic_generator.generate()
    
    # Step 4: Add synthetic examples to the dataset
    data_processor.add_examples(synthetic_examples, "train")
    
    # Finalize the dataset
    Let final_dataset = data_processor.finalize()
    
    # Step 5: Analyze the dataset
    Print("Analyzing final dataset...")
    Let data_analyzer = DatasetAnalyzer.create({
        "dataset": final_dataset
    })
    
    Let analysis = data_analyzer.analyze()
    
    # Print analysis
    Print("Dataset analysis:")
    Print("Total examples: " + analysis.total_examples)
    Print("Train/validation/test split: " + analysis.split_ratio.join("/"))
    Print("Average input length: " + analysis.average_input_length + " tokens")
    Print("Average output length: " + analysis.average_output_length + " tokens")
    Print("Topic distribution:")
    For topic in analysis.topic_distribution.keys():
        Print("- " + topic + ": " + analysis.topic_distribution[topic] + "%")
    
    # Save the dataset
    data_processor.save("./processed_data/final_dataset/")
    
    Print("Dataset created and saved to ./processed_data/final_dataset/")
    
    Return {
        "dataset": final_dataset,
        "analysis": analysis,
        "path": "./processed_data/final_dataset/"
    }

# Example 6: Fine-tuning a specialized code completion model
Process called "fine_tune_code_completion":
    # Step 1: Prepare code completion dataset
    Print("Preparing code completion dataset...")
    Let completion_dataset = CodeCompletionDatasetBuilder.create({
        "source_directory": "./src/",
        "file_extensions": [".runa"],
        "exclude_patterns": ["**/test/**"],
        "completion_strategies": [
            {"type": "line_completion", "context_lines": 10, "completion_lines": 1},
            {"type": "function_completion", "context_percent": 30},
            {"type": "argument_completion", "context_type": "function_signature"}
        ],
        "validation_split": 0.1
    })
    
    Let datasets = completion_dataset.build()
    
    # Step 2: Load base model suitable for completion
    Print("Loading base model...")
    Let model_loader = ModelLoader.create({
        "model_type": "llm",
        "model_id": "runa-code-completion-base",
        "device": "auto"
    })
    
    Let base_model = model_loader.load_for_fine_tuning({
        "adapter_type": "lora",
        "trainable_components": ["attention", "mlp"],
        "rank": 16
    })
    
    # Step 3: Configure training for completion task
    Print("Configuring training...")
    Let training_config = TrainingConfigurator.create({
        "training_objective": "completion",
        "optimization": {
            "optimizer": "adamw",
            "learning_rate": 5e-5,
            "lr_scheduler": "cosine",
            "warmup_ratio": 0.1
        },
        "training_parameters": {
            "batch_size": 8,
            "epochs": 5,
            "max_length": 512
        }
    })
    
    # Step 4: Run fine-tuning
    Print("Starting code completion fine-tuning...")
    Let fine_tuner = FineTuner.create({
        "model": base_model,
        "datasets": datasets,
        "training_config": training_config,
        "output_dir": "./models/code_completion/",
        "evaluation_frequency": 500
    })
    
    Let training_result = fine_tuner.train()
    
    # Step 5: Evaluate code completion specifically
    Print("Evaluating code completion model...")
    Let completion_evaluator = CodeCompletionEvaluator.create({
        "model": training_result.model,
        "test_data": datasets.validation,
        "metrics": ["exact_match", "token_accuracy", "syntax_validity", "semantic_accuracy"]
    })
    
    Let completion_results = completion_evaluator.evaluate()
    
    # Step 6: Export model for IDE integration
    Print("Exporting model for IDE integration...")
    Let exporter = ModelExporter.create({
        "model": training_result.model,
        "output_format": "onnx",
        "quantization": "int8",
        "optimization_level": "o2"
    })
    
    Let export_path = exporter.export("./models/exported/code_completion/")
    
    # Create a deployment package for IDE
    Let ide_package = exporter.create_deployment_package({
        "target": "ide",
        "include_sample_config": true,
        "compression": "zip"
    })
    
    # Print results
    Print("Code completion model training completed!")
    Print("Token accuracy: " + completion_results.token_accuracy + "%")
    Print("Syntax validity: " + completion_results.syntax_validity + "%")
    Print("Exported model size: " + ide_package.size + " MB")
    Print("IDE package created at: " + ide_package.path)
    
    Return {
        "model": training_result.model,
        "evaluation": completion_results,
        "export_path": export_path,
        "ide_package": ide_package
    }

# Example 7: Complete production fine-tuning pipeline
Process called "production_fine_tuning_pipeline":
    # Step 1: Load configuration
    Print("Loading fine-tuning configuration...")
    Let config_loader = ConfigurationLoader.create("./config/fine_tuning_config.json")
    Let config = config_loader.load()
    
    # Step 2: Set up experiment tracking
    Print("Setting up experiment tracking...")
    Let experiment = ExperimentTracker.create({
        "name": config.experiment_name,
        "tracking_uri": config.tracking_uri,
        "tags": config.tags
    })
    
    experiment.start()
    
    # Step 3: Prepare datasets
    Print("Preparing datasets...")
    Let data_processor = TrainingDataProcessor.create({
        "config": config.data_processing,
        "version_tracking": true
    })
    
    Let datasets = data_processor.process()
    experiment.log_dataset_info(datasets)
    
    # Step 4: Set up model
    Print("Setting up model...")
    Let model_manager = ModelManager.create({
        "config": config.model,
        "experiment": experiment
    })
    
    Let base_model = model_manager.load_base_model()
    
    # Step 5: Configure training stages
    Print("Configuring training stages...")
    Let pipeline = FineTuningPipeline.create({
        "stages": config.training_stages,
        "model": base_model,
        "datasets": datasets,
        "experiment": experiment
    })
    
    # Step 6: Execute training pipeline
    Print("Executing training pipeline...")
    Let pipeline_result = pipeline.execute()
    
    # Step 7: Evaluate final model
    Print("Evaluating final model...")
    Let evaluator = ModelEvaluator.create({
        "model": pipeline_result.final_model,
        "datasets": datasets.test,
        "metrics": config.evaluation.metrics
    })
    
    Let evaluation_results = evaluator.evaluate()
    experiment.log_metrics(evaluation_results)
    
    # Step 8: Export final model
    Print("Exporting final model...")
    Let exporter = ModelExporter.create({
        "model": pipeline_result.final_model,
        "config": config.export
    })
    
    Let export_result = exporter.export()
    
    # Step 9: Generate model card and documentation
    Print("Generating model documentation...")
    Let documentor = ModelDocumentor.create({
        "model": pipeline_result.final_model,
        "training_history": pipeline_result.training_history,
        "evaluation_results": evaluation_results,
        "config": config
    })
    
    Let model_card = documentor.generate_model_card()
    Let user_guide = documentor.generate_user_guide()
    
    documentor.save_documentation("./models/exported/docs/")
    
    # Step 10: Finish experiment
    experiment.finish()
    
    # Print summary
    Print("Fine-tuning pipeline completed successfully!")
    Print("Experiment ID: " + experiment.id)
    Print("Final model path: " + export_result.path)
    Print("Documentation path: ./models/exported/docs/")
    
    Print("Key metrics:")
    For metric_name in evaluation_results.keys():
        Print("- " + metric_name + ": " + evaluation_results[metric_name])
    
    Return {
        "model": pipeline_result.final_model,
        "experiment_id": experiment.id,
        "model_path": export_result.path,
        "evaluation": evaluation_results,
        "documentation": {
            "model_card": model_card,
            "user_guide": user_guide
        }
    }

# Example 8: Fine-tuning a specialized embedding model
Process called "fine_tune_embedding_model":
    # Step 1: Prepare embedding training data
    Print("Preparing embedding training data...")
    Let embedding_dataset = EmbeddingDatasetBuilder.create({
        "source_directory": "./src/",
        "documentation_directory": "./docs/",
        "positive_pair_strategies": [
            {"type": "function_docstring"},
            {"type": "similar_functions"},
            {"type": "code_explanation"}
        ],
        "contrastive_sampling": true,
        "validation_split": 0.1
    })
    
    Let datasets = embedding_dataset.build()
    
    # Step 2: Load base embedding model
    Print("Loading base embedding model...")
    Let model_loader = ModelLoader.create({
        "model_type": "embedding",
        "model_id": "runa-code-embedding-base",
        "device": "auto"
    })
    
    Let base_model = model_loader.load_for_fine_tuning()
    
    # Step 3: Configure embedding training
    Print("Configuring embedding training...")
    Let training_config = EmbeddingTrainingConfigurator.create({
        "training_objective": "contrastive",
        "loss_function": "cosine_similarity",
        "negative_sampling": {
            "strategy": "hard_negatives",
            "count": 5
        },
        "optimization": {
            "optimizer": "adamw",
            "learning_rate": 1e-5,
            "weight_decay": 0.01
        },
        "training_parameters": {
            "batch_size": 64,
            "epochs": 10
        }
    })
    
    # Step 4: Run embedding fine-tuning
    Print("Starting embedding model fine-tuning...")
    Let fine_tuner = EmbeddingFineTuner.create({
        "model": base_model,
        "datasets": datasets,
        "training_config": training_config,
        "output_dir": "./models/code_embedding/",
        "evaluation_frequency": 200
    })
    
    Let training_result = fine_tuner.train()
    
    # Step 5: Evaluate embedding model
    Print("Evaluating embedding model...")
    Let embedding_evaluator = EmbeddingEvaluator.create({
        "model": training_result.model,
        "test_data": datasets.validation,
        "evaluation_tasks": [
            "semantic_code_search",
            "similar_code_detection",
            "code_clustering"
        ]
    })
    
    Let embedding_results = embedding_evaluator.evaluate()
    
    # Step 6: Export embedding model
    Print("Exporting embedding model...")
    Let exporter = ModelExporter.create({
        "model": training_result.model,
        "output_format": "onnx",
        "optimization_level": "o1"
    })
    
    Let export_path = exporter.export("./models/exported/code_embedding/")
    
    # Print results
    Print("Embedding model training completed!")
    Print("Semantic search precision@10: " + embedding_results.semantic_code_search.precision_at_10)
    Print("Similar code detection accuracy: " + embedding_results.similar_code_detection.accuracy)
    Print("Code clustering v-measure: " + embedding_results.code_clustering.v_measure)
    
    Return {
        "model": training_result.model,
        "evaluation": embedding_results,
        "export_path": export_path
    }