# LLM Integration Examples
# This file demonstrates how Runa can be integrated with Large Language Models

import runa.ai.llm
import runa.ai.prompt_engineering
import runa.ai.model_evaluation

# Example 1: Basic LLM interaction
Process called "simple_llm_completion":
    # Initialize LLM connection
    Let model = LLM.connect("runa_assistant_model")
    
    # Send a simple completion request
    Let response = model.complete("Write a function in Runa to calculate factorial")
    
    Print("LLM Response:")
    Print(response)
    
    # You can also execute code generated by the LLM
    Let executable_code = model.complete_as_code("Write a function to calculate factorial of n")
    
    # Safely execute the generated code in a sandbox
    Let result = LLM.execute_safely(executable_code, {"n": 5})
    Print("Factorial of 5: " + result)
    
    Return response

# Example 2: Advanced prompting techniques
Process called "advanced_prompting":
    # Create a prompt template
    Let template = PromptTemplate.create(
        "Generate a {{language}} function that {{task}} with the following requirements: {{requirements}}"
    )
    
    # Fill the template with values
    Let prompt = template.fill({
        "language": "Runa",
        "task": "sorts a list of integers",
        "requirements": "- Must use quicksort algorithm\n- Should handle empty lists\n- Must include comments"
    })
    
    # Connect to LLM and send the prompt
    Let model = LLM.connect("runa_assistant_model")
    Let response = model.complete(prompt)
    
    Print("Generated code based on requirements:")
    Print(response)
    
    Return response

# Example 3: Few-shot learning examples
Process called "few_shot_learning":
    # Create examples for few-shot learning
    Let examples = [
        {
            "input": "Sort this list: [3, 1, 5, 2]",
            "output": "[1, 2, 3, 5]"
        },
        {
            "input": "Find maximum: [7, 2, 9, 4]",
            "output": "9"
        },
        {
            "input": "Count elements: [a, b, c, d, e]",
            "output": "5"
        }
    ]
    
    # Create a few-shot prompt
    Let few_shot_prompt = FewShotPrompt.create(
        examples,
        prefix="Solve the following problem:",
        suffix="{{input}}"
    )
    
    # Connect to LLM and use few-shot learning
    Let model = LLM.connect("runa_assistant_model")
    Let result = model.complete(few_shot_prompt.format({"input": "Reverse this list: [1, 2, 3, 4]"}))
    
    Print("Few-shot learning result:")
    Print(result)  # Expected output: [4, 3, 2, 1]
    
    Return result

# Example 4: Chain-of-thought prompting
Process called "chain_of_thought":
    # Create a chain-of-thought prompt
    Let cot_prompt = ChainOfThoughtPrompt.create(
        "Problem: {{problem}}\n\nLet's think about this step by step:",
        "First, {{step1}}\nThen, {{step2}}\nFinally, {{step3}}\n\nThe answer is: {{answer}}"
    )
    
    # Connect to LLM and use chain of thought reasoning
    Let model = LLM.connect("runa_assistant_model")
    Let problem = "If 5 apples cost $10, how much do 8 apples cost?"
    
    Let response = model.complete_with_cot(problem)
    
    Print("Chain of thought reasoning:")
    Print(response.reasoning)
    Print("Final answer: " + response.answer)
    
    Return response

# Example 5: LLM-assisted code generation
Process called "llm_assisted_coding" that takes code_description:String:
    # Initialize code generator
    Let code_generator = CodeGenerator.create()
    
    # Generate code based on description
    Let generated_code = code_generator.generate(code_description)
    
    # Validate the generated code
    Let validation_result = code_generator.validate(generated_code)
    
    If not validation_result.is_valid:
        # Automatically fix issues
        generated_code = code_generator.fix(generated_code, validation_result.issues)
    
    # Execute tests on the generated code
    Let test_results = code_generator.test(generated_code)
    
    Return {
        "code": generated_code,
        "validation": validation_result,
        "tests": test_results
    }

# Example 6: Model evaluation
Process called "evaluate_model_performance":
    # Create an evaluator
    Let evaluator = ModelEvaluator.create()
    
    # Load test dataset
    Let test_data = Dataset.load("runa_code_generation_benchmark")
    
    # Connect to multiple models for comparison
    Let models = [
        LLM.connect("runa_assistant_v1"),
        LLM.connect("runa_assistant_v2"),
        LLM.connect("external_code_model")
    ]
    
    # Evaluate models on the test data
    Let evaluation_results = evaluator.evaluate_all(models, test_data, {
        "metrics": ["accuracy", "code_correctness", "execution_time"],
        "sample_size": 100
    })
    
    # Generate comparison report
    Let report = evaluator.generate_report(evaluation_results)
    
    Print("Model Evaluation Report:")
    Print(report.summary)
    
    Return evaluation_results 